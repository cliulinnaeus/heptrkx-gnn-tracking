{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/bin/python\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets.graph import load_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare network-x graphs using hitsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/global/cscratch1/sd/xju/heptrkx/data/hitgraphs_001/event00000{}_g{:03d}.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(in_node, out_node):\n",
    "    # input are the features of incoming and outgoing nodes\n",
    "    # they are ordered as [r, phi, z]\n",
    "    in_r, in_phi, _   = in_node\n",
    "    out_r, out_phi, _ = out_node\n",
    "    in_x = in_r * np.cos(in_phi)\n",
    "    in_y = in_r * np.sin(in_phi)\n",
    "    out_x = out_r * np.cos(out_phi)\n",
    "    out_y = out_r * np.sin(out_phi)\n",
    "    return np.sqrt((in_x - out_x)**2 + (in_y - out_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_graph_from_hitsgraph(ievt, isec):\n",
    "    ## ievt start from 1000\n",
    "    file_name = base_dir.format(ievt, isec)\n",
    "    return hitsgraph_to_networkx_graph(load_graph(file_name))\n",
    "\n",
    "\n",
    "def hitsgraph_to_networkx_graph(G):\n",
    "    n_nodes, n_edges = G.Ri.shape\n",
    "    \n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    ## add nodes\n",
    "    for i in range(n_nodes):\n",
    "        graph.add_node(i, pos=G.X[i], solution=0.0)\n",
    "    \n",
    "    for iedge in range(n_edges):\n",
    "        in_node_id  = G.Ri[:, iedge].nonzero()[0][0]\n",
    "        out_node_id = G.Ro[:, iedge].nonzero()[0][0]\n",
    "\n",
    "        # distance as features\n",
    "        in_node_features  = G.X[in_node_id]\n",
    "        out_node_features = G.X[out_node_id]\n",
    "        distance = get_edge_features(in_node_features, out_node_features)\n",
    "        # add edges, bi-directions\n",
    "        graph.add_edge(in_node_id, out_node_id, distance=distance, solution=G.y[iedge])\n",
    "        graph.add_edge(out_node_id, in_node_id, distance=distance, solution=G.y[iedge])\n",
    "        # add \"solution\" to nodes\n",
    "        graph.node[in_node_id].update(solution=G.y[iedge])\n",
    "        graph.node[out_node_id].update(solution=G.y[iedge])\n",
    "        \n",
    "        \n",
    "    # add global features, not used for now\n",
    "    graph.graph['features'] = np.array([0.])\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def graph_to_input_target(graph):\n",
    "    def create_feature(attr, fields):\n",
    "        return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "    \n",
    "    input_node_fields = (\"pos\",)\n",
    "    input_edge_fields = (\"distance\",)\n",
    "    target_node_fields = (\"solution\",)\n",
    "    target_edge_fields = (\"solution\",)\n",
    "    \n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "    \n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "        input_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, input_node_fields)\n",
    "        )\n",
    "        target_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, target_node_fields)\n",
    "        )\n",
    "        \n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "        input_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, input_edge_fields)\n",
    "        )\n",
    "        target_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, target_edge_fields)\n",
    "        )\n",
    "        \n",
    "    input_graph.graph['features'] = input_graph.graph['features'] = np.array([0.0])\n",
    "    return input_graph, target_graph\n",
    "\n",
    "\n",
    "def generate_input_target(n_graphs, start_evt_id=1000):\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    for i in range(n_graphs):\n",
    "        evt_id = start_evt_id + i\n",
    "        isec = 0\n",
    "        graph = networkx_graph_from_hitsgraph(evt_id, isec)\n",
    "        input_graph, output_graph = graph_to_input_target(graph)\n",
    "        input_graphs.append(input_graph)\n",
    "        target_graphs.append(output_graph)\n",
    "    return input_graphs, target_graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use interaction network from modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_nets import modules\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.\n",
    "\n",
    "\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "\n",
    "  Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "  \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "  def __init__(self, name=\"MLPGraphIndependent\"):\n",
    "    super(MLPGraphIndependent, self).__init__(name=name)\n",
    "    with self._enter_variable_scope():\n",
    "      self._network = modules.GraphIndependent(\n",
    "          edge_model_fn=make_mlp_model,\n",
    "          node_model_fn=make_mlp_model,\n",
    "          global_model_fn=None)\n",
    "\n",
    "  def _build(self, inputs):\n",
    "    return self._network(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class SegmentClassifier(snt.AbstractModule):\n",
    "\n",
    "  def __init__(self, name=\"SegmentClassifier\"):\n",
    "    super(SegmentClassifier, self).__init__(name=name)\n",
    "\n",
    "    self._encoder = MLPGraphIndependent()\n",
    "    self._core = modules.InteractionNetwork(\n",
    "        edge_model_fn=make_mlp_model,\n",
    "        node_model_fn=make_mlp_model,\n",
    "        reducer=tf.unsorted_segment_prod\n",
    "    )\n",
    "\n",
    "    # Transforms the outputs into the appropriate shapes.\n",
    "    edge_output_size = 1\n",
    "#     edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "    edge_fn =lambda: snt.nets.MLP([edge_output_size], activation=tf.nn.tanh, name='edge_output')\n",
    "\n",
    "    with self._enter_variable_scope():\n",
    "      self._output_transform = modules.GraphIndependent(edge_fn, None, None)\n",
    "\n",
    "  def _build(self, input_op, num_processing_steps):\n",
    "    latent = self._encoder(input_op)\n",
    "\n",
    "    output_ops = []\n",
    "    for _ in range(num_processing_steps):\n",
    "        core_input = latent\n",
    "        latent = self._core(core_input)\n",
    "        output_ops.append(self._output_transform(latent))\n",
    "    return output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Loss functions and Feed-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global event_track\n",
    "event_track = 1000\n",
    "def create_feed_dict(batch_size, input_ph, target_ph):\n",
    "    global event_track\n",
    "    inputs, targets = generate_input_target(batch_size, event_track)\n",
    "    event_track += batch_size\n",
    "\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "#         print(event_track)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_ops(target_op, output_ops):\n",
    "    # only use edges\n",
    "    loss_ops = [\n",
    "        tf.losses.sigmoid_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "  \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "  return [utils_tf.make_runnable_in_session(a) for a in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_matrics(target, output):\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    \n",
    "    test_target = []\n",
    "    test_pred = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        test_target.append(td['edges'])\n",
    "        test_pred.append(od['edges'])\n",
    "    \n",
    "    test_target = np.concatenate(test_target, axis=0)\n",
    "    test_pred   = np.concatenate(test_pred,   axis=0)\n",
    "    \n",
    "    thresh = 0.5\n",
    "    y_pred, y_true = (test_pred > thresh), (test_target > thresh)\n",
    "    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = n_graphs = 32\n",
    "num_training_iterations = 10000\n",
    "num_processing_steps_tr = 10  ## level of message-passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graphs, target_graphs = generate_input_target(n_graphs)\n",
    "input_ph  = utils_tf.placeholders_from_networkxs(input_graphs, force_dynamic_num_graphs=True)\n",
    "target_ph = utils_tf.placeholders_from_networkxs(target_graphs, force_dynamic_num_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reset session  { form-width: \"30%\" }\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "\n",
    "\n",
    "#@title Run training  { form-width: \"30%\" }\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), \"\n",
    "      \"Precision, \"\n",
    "      \"Recall\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"target\": target_ph,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  }, feed_dict=feed_dict)\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "    test_values = sess.run({\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "    correct_tr, solved_tr = computer_matrics(\n",
    "        test_values[\"target\"], test_values[\"outputs\"][-1])\n",
    "    elapsed = time.time() - start_time\n",
    "    losses_tr.append(train_values[\"loss\"])\n",
    "    corrects_tr.append(correct_tr)\n",
    "    solveds_tr.append(solved_tr)\n",
    "    logged_iterations.append(iteration)\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall\"\n",
    "          \" {:.4f}\".format(\n",
    "              iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "              correct_tr, solved_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrackPy3.6",
   "language": "python",
   "name": "trackkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/bin/python\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets.graph import load_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare network-x graphs using hitsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/global/cscratch1/sd/xju/heptrkx/data/hitgraphs_001/event00000{}_g{:03d}.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(in_node, out_node):\n",
    "    # input are the features of incoming and outgoing nodes\n",
    "    # they are ordered as [r, phi, z]\n",
    "    in_r, in_phi, _   = in_node\n",
    "    out_r, out_phi, _ = out_node\n",
    "    in_x = in_r * np.cos(in_phi)\n",
    "    in_y = in_r * np.sin(in_phi)\n",
    "    out_x = out_r * np.cos(out_phi)\n",
    "    out_y = out_r * np.sin(out_phi)\n",
    "    return np.sqrt((in_x - out_x)**2 + (in_y - out_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_graph_from_hitsgraph(ievt, isec):\n",
    "    ## ievt start from 1000\n",
    "    file_name = base_dir.format(ievt, isec)\n",
    "    return hitsgraph_to_networkx_graph(load_graph(file_name))\n",
    "\n",
    "\n",
    "def hitsgraph_to_networkx_graph(G):\n",
    "    n_nodes, n_edges = G.Ri.shape\n",
    "    \n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    ## add nodes\n",
    "    for i in range(n_nodes):\n",
    "        graph.add_node(i, pos=G.X[i], solution=0.0)\n",
    "    \n",
    "    for iedge in range(n_edges):\n",
    "        in_node_id  = G.Ri[:, iedge].nonzero()[0][0]\n",
    "        out_node_id = G.Ro[:, iedge].nonzero()[0][0]\n",
    "\n",
    "        # distance as features\n",
    "        in_node_features  = G.X[in_node_id]\n",
    "        out_node_features = G.X[out_node_id]\n",
    "        distance = get_edge_features(in_node_features, out_node_features)\n",
    "        # add edges, bi-directions\n",
    "        graph.add_edge(in_node_id, out_node_id, distance=distance, solution=G.y[iedge])\n",
    "        graph.add_edge(out_node_id, in_node_id, distance=distance, solution=G.y[iedge])\n",
    "        # add \"solution\" to nodes\n",
    "        graph.node[in_node_id].update(solution=G.y[iedge])\n",
    "        graph.node[out_node_id].update(solution=G.y[iedge])\n",
    "        \n",
    "        \n",
    "    # add global features, not used for now\n",
    "    graph.graph['features'] = np.array([0.])\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def graph_to_input_target(graph):\n",
    "    def create_feature(attr, fields):\n",
    "        return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "    \n",
    "    input_node_fields = (\"pos\",)\n",
    "    input_edge_fields = (\"distance\",)\n",
    "    target_node_fields = (\"solution\",)\n",
    "    target_edge_fields = (\"solution\",)\n",
    "    \n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "    \n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "        input_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, input_node_fields)\n",
    "        )\n",
    "        target_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, target_node_fields)\n",
    "        )\n",
    "        \n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "        input_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, input_edge_fields)\n",
    "        )\n",
    "        target_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, target_edge_fields)\n",
    "        )\n",
    "        \n",
    "    input_graph.graph['features'] = input_graph.graph['features'] = np.array([0.0])\n",
    "    return input_graph, target_graph\n",
    "\n",
    "\n",
    "def generate_input_target(n_graphs, start_evt_id=1000):\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    for i in range(n_graphs):\n",
    "        evt_id = start_evt_id + i\n",
    "        isec = 0\n",
    "        graph = networkx_graph_from_hitsgraph(evt_id, isec)\n",
    "        input_graph, output_graph = graph_to_input_target(graph)\n",
    "        input_graphs.append(input_graph)\n",
    "        target_graphs.append(output_graph)\n",
    "    return input_graphs, target_graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use interaction network from modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_nets import modules\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.\n",
    "\n",
    "\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "\n",
    "  Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "  \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "  def __init__(self, name=\"MLPGraphIndependent\"):\n",
    "    super(MLPGraphIndependent, self).__init__(name=name)\n",
    "    with self._enter_variable_scope():\n",
    "      self._network = modules.GraphIndependent(\n",
    "          edge_model_fn=make_mlp_model,\n",
    "          node_model_fn=make_mlp_model,\n",
    "          global_model_fn=None)\n",
    "\n",
    "  def _build(self, inputs):\n",
    "    return self._network(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class SegmentClassifier(snt.AbstractModule):\n",
    "\n",
    "  def __init__(self, name=\"SegmentClassifier\"):\n",
    "    super(SegmentClassifier, self).__init__(name=name)\n",
    "\n",
    "    self._encoder = MLPGraphIndependent()\n",
    "    self._core = modules.InteractionNetwork(\n",
    "        edge_model_fn=make_mlp_model,\n",
    "        node_model_fn=make_mlp_model,\n",
    "        reducer=tf.unsorted_segment_prod\n",
    "    )\n",
    "\n",
    "    # Transforms the outputs into the appropriate shapes.\n",
    "    edge_output_size = 1\n",
    "#     edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "    edge_fn =lambda: snt.nets.MLP([edge_output_size], activation=tf.nn.tanh, name='edge_output')\n",
    "\n",
    "    with self._enter_variable_scope():\n",
    "      self._output_transform = modules.GraphIndependent(edge_fn, None, None)\n",
    "\n",
    "  def _build(self, input_op, num_processing_steps):\n",
    "    latent = self._encoder(input_op)\n",
    "\n",
    "    output_ops = []\n",
    "    for _ in range(num_processing_steps):\n",
    "        core_input = latent\n",
    "        latent = self._core(core_input)\n",
    "        output_ops.append(self._output_transform(latent))\n",
    "    return output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SegmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Loss functions and Feed-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global event_track\n",
    "event_track = 1000\n",
    "def create_feed_dict(batch_size, input_ph, target_ph):\n",
    "    global event_track\n",
    "    inputs, targets = generate_input_target(batch_size, event_track)\n",
    "    event_track += batch_size\n",
    "\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "#         print(event_track)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_ops(target_op, output_ops):\n",
    "    # only use edges\n",
    "    loss_ops = [\n",
    "        tf.losses.sigmoid_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "  \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "  return [utils_tf.make_runnable_in_session(a) for a in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_matrics(target, output):\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    \n",
    "    test_target = []\n",
    "    test_pred = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        test_target.append(td['edges'])\n",
    "        test_pred.append(od['edges'])\n",
    "    \n",
    "    test_target = np.concatenate(test_target, axis=0)\n",
    "    test_pred   = np.concatenate(test_pred,   axis=0)\n",
    "    \n",
    "    thresh = 0.5\n",
    "    y_pred, y_true = (test_pred > thresh), (test_target > thresh)\n",
    "    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = n_graphs = 2\n",
    "num_training_iterations = 10000\n",
    "num_processing_steps_tr = 4  ## level of message-passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graphs, target_graphs = generate_input_target(n_graphs)\n",
    "input_ph  = utils_tf.placeholders_from_networkxs(input_graphs, force_dynamic_num_graphs=True)\n",
    "target_ph = utils_tf.placeholders_from_networkxs(target_graphs, force_dynamic_num_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\n",
      "# 00013, T 22.8, Ltr 0.6372, Lge 0.6301, Precision 0.6904, Recall 0.0293\n",
      "# 00027, T 43.2, Ltr 0.5904, Lge 0.6244, Precision 0.8113, Recall 0.0616\n",
      "# 00042, T 65.1, Ltr 0.5759, Lge 0.5341, Precision 0.7420, Recall 0.0686\n",
      "# 00057, T 86.2, Ltr 0.5313, Lge 0.5171, Precision 0.7730, Recall 0.1054\n",
      "# 00072, T 105.6, Ltr 0.5454, Lge 0.5505, Precision 0.7383, Recall 0.2142\n",
      "# 00086, T 126.4, Ltr 0.5515, Lge 0.4892, Precision 0.7317, Recall 0.1829\n",
      "# 00099, T 146.5, Ltr 0.5668, Lge 0.5344, Precision 0.7743, Recall 0.2098\n",
      "# 00113, T 166.9, Ltr 0.5395, Lge 0.5237, Precision 0.7402, Recall 0.2023\n",
      "# 00127, T 187.4, Ltr 0.4933, Lge 0.5168, Precision 0.7376, Recall 0.1599\n",
      "# 00140, T 207.3, Ltr 0.5417, Lge 0.5197, Precision 0.7889, Recall 0.2034\n",
      "# 00154, T 228.0, Ltr 0.5143, Lge 0.4965, Precision 0.8231, Recall 0.1819\n",
      "# 00167, T 248.5, Ltr 0.5001, Lge 0.5246, Precision 0.7828, Recall 0.2045\n",
      "# 00182, T 271.1, Ltr 0.4934, Lge 0.4800, Precision 0.7671, Recall 0.1466\n",
      "# 00194, T 290.3, Ltr 0.4764, Lge 0.5167, Precision 0.7892, Recall 0.2002\n",
      "# 00207, T 311.1, Ltr 0.5184, Lge 0.5019, Precision 0.8135, Recall 0.2168\n"
     ]
    }
   ],
   "source": [
    "#@title Reset session  { form-width: \"30%\" }\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "\n",
    "\n",
    "#@title Run training  { form-width: \"30%\" }\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), \"\n",
    "      \"Precision, \"\n",
    "      \"Recall\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"target\": target_ph,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  }, feed_dict=feed_dict)\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "    test_values = sess.run({\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "    correct_tr, solved_tr = computer_matrics(\n",
    "        test_values[\"target\"], test_values[\"outputs\"][-1])\n",
    "    elapsed = time.time() - start_time\n",
    "    losses_tr.append(train_values[\"loss\"])\n",
    "    corrects_tr.append(correct_tr)\n",
    "    solveds_tr.append(solved_tr)\n",
    "    logged_iterations.append(iteration)\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall\"\n",
    "          \" {:.4f}\".format(\n",
    "              iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "              correct_tr, solved_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(target, output):\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    \n",
    "    cs = []\n",
    "    ss = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        xe = np.argmax(td['edges'], axis=-1)\n",
    "        ye = np.argmax(od['edges'], axis=-1)\n",
    "        c = []\n",
    "        c.append(xe == ye)\n",
    "        c = np.concatenate(c, axis=0)\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "    return correct, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "output_sizes must be iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6e8f71213eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegmentClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-cfa7a75613e2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0medge_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#     edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0medge_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_output_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enter_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/sonnet/python/modules/nets/mlp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_sizes, activation, activate_final, initializers, partitioners, regularizers, use_bias, custom_getter, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_sizes must be iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0moutput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: output_sizes must be iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   86   92 ... 2402 2417 2632]\n",
      "(2687, 3)\n",
      "(16504, 1)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "graph_tuple = utils_np.networkxs_to_graphs_tuple(input_graphs)\n",
    "print(graph_tuple.receivers)\n",
    "print(graph_tuple.nodes.shape)\n",
    "print(graph_tuple.edges.shape)\n",
    "print(graph_tuple.receivers[np.where(graph_tuple.receivers > graph_tuple.nodes.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Accuracy, Recall\n",
      "# 00008, T 25.1, Ltr 0.5930, Lge 0.5660, Accuracy 0.3445, Recall 0.0086\n",
      "# 00015, T 46.1, Ltr 0.5734, Lge 0.5504, Accuracy 0.2917, Recall 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00027, T 66.6, Ltr 0.5056, Lge 0.5433, Accuracy 0.0000, Recall 0.0000\n",
      "# 00040, T 87.9, Ltr 0.5115, Lge 0.5213, Accuracy 1.0000, Recall 0.0035\n",
      "# 00052, T 108.5, Ltr 0.5266, Lge 0.5079, Accuracy 0.9510, Recall 0.0672\n",
      "# 00065, T 129.5, Ltr 0.5020, Lge 0.4900, Accuracy 0.9028, Recall 0.1077\n",
      "# 00078, T 149.6, Ltr 0.5047, Lge 0.5065, Accuracy 0.8380, Recall 0.2242\n",
      "# 00091, T 170.4, Ltr 0.4696, Lge 0.5240, Accuracy 0.8215, Recall 0.2143\n",
      "# 00104, T 191.8, Ltr 0.4965, Lge 0.5122, Accuracy 0.8042, Recall 0.2002\n",
      "# 00118, T 212.3, Ltr 0.4965, Lge 0.5194, Accuracy 0.8117, Recall 0.2171\n",
      "# 00132, T 232.2, Ltr 0.5041, Lge 0.5337, Accuracy 0.8201, Recall 0.2896\n",
      "# 00146, T 252.2, Ltr 0.5013, Lge 0.5338, Accuracy 0.8260, Recall 0.2709\n",
      "# 00160, T 273.6, Ltr 0.5007, Lge 0.4995, Accuracy 0.8578, Recall 0.2207\n",
      "# 00174, T 293.7, Ltr 0.5322, Lge 0.5083, Accuracy 0.8453, Recall 0.2218\n",
      "# 00185, T 314.5, Ltr 0.4681, Lge 0.4768, Accuracy 0.8492, Recall 0.1950\n",
      "# 00199, T 334.8, Ltr 0.5020, Lge 0.5120, Accuracy 0.8589, Recall 0.2236\n",
      "# 00208, T 355.9, Ltr 0.5041, Lge 0.4974, Accuracy 0.8377, Recall 0.2180\n",
      "# 00222, T 376.8, Ltr 0.5122, Lge 0.5065, Accuracy 0.8366, Recall 0.2337\n",
      "# 00235, T 397.4, Ltr 0.4925, Lge 0.5215, Accuracy 0.8173, Recall 0.2483\n",
      "# 00248, T 418.8, Ltr 0.4667, Lge 0.4822, Accuracy 0.7996, Recall 0.1866\n",
      "# 00261, T 439.8, Ltr 0.5019, Lge 0.4666, Accuracy 0.8225, Recall 0.2117\n",
      "# 00274, T 461.0, Ltr 0.4635, Lge 0.4980, Accuracy 0.8558, Recall 0.2276\n",
      "# 00286, T 481.7, Ltr 0.4970, Lge 0.4937, Accuracy 0.8283, Recall 0.1944\n",
      "# 00299, T 502.9, Ltr 0.4833, Lge 0.5029, Accuracy 0.8430, Recall 0.2252\n",
      "# 00312, T 523.4, Ltr 0.5287, Lge 0.5017, Accuracy 0.8065, Recall 0.2780\n",
      "# 00323, T 543.6, Ltr 0.5193, Lge 0.5022, Accuracy 0.8645, Recall 0.2214\n",
      "# 00334, T 563.8, Ltr 0.5122, Lge 0.5314, Accuracy 0.8837, Recall 0.2715\n",
      "# 00345, T 584.0, Ltr 0.5034, Lge 0.5286, Accuracy 0.8756, Recall 0.2819\n",
      "# 00356, T 604.8, Ltr 0.4989, Lge 0.5072, Accuracy 0.8696, Recall 0.2464\n",
      "# 00367, T 625.5, Ltr 0.4927, Lge 0.4679, Accuracy 0.8577, Recall 0.1777\n",
      "# 00379, T 646.3, Ltr 0.4778, Lge 0.5119, Accuracy 0.8762, Recall 0.1907\n",
      "# 00390, T 666.8, Ltr 0.4912, Lge 0.5024, Accuracy 0.8369, Recall 0.2352\n",
      "# 00404, T 687.9, Ltr 0.4597, Lge 0.5186, Accuracy 0.8842, Recall 0.2514\n",
      "# 00417, T 708.4, Ltr 0.5210, Lge 0.4983, Accuracy 0.8148, Recall 0.2411\n",
      "# 00430, T 729.8, Ltr 0.5133, Lge 0.4713, Accuracy 0.8498, Recall 0.1886\n",
      "# 00441, T 751.2, Ltr 0.4798, Lge 0.4814, Accuracy 0.8539, Recall 0.1997\n",
      "# 00453, T 771.9, Ltr 0.4924, Lge 0.5078, Accuracy 0.8513, Recall 0.2296\n",
      "# 00465, T 792.9, Ltr 0.5168, Lge 0.4992, Accuracy 0.8639, Recall 0.2255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-667b8fa9f8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mlast_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   train_values = sess.run({\n\u001b[1;32m     40\u001b[0m       \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-db5bc38f93c5>\u001b[0m in \u001b[0;36mcreate_feed_dict\u001b[0;34m(batch_size, input_ph, target_ph)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mevent_track\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_input_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mevent_track\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-41af32b244e6>\u001b[0m in \u001b[0;36mgenerate_input_target\u001b[0;34m(n_graphs, start_evt_id)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mevt_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_evt_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0misec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworkx_graph_from_hitsgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0minput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_to_input_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0minput_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-41af32b244e6>\u001b[0m in \u001b[0;36mnetworkx_graph_from_hitsgraph\u001b[0;34m(ievt, isec)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m## ievt start from 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mievt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhitsgraph_to_networkx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-41af32b244e6>\u001b[0m in \u001b[0;36mhitsgraph_to_networkx_graph\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# add \"solution\" to nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_node_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miedge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_node_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miedge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = train_values['outputs'][-1]\n",
    "odd = utils_np.graphs_tuple_to_data_dicts(output)[0]\n",
    "\n",
    "tdd = utils_np.graphs_tuple_to_data_dicts(train_values['target'])[0]\n",
    "\n",
    "tdd['edges'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34815952],\n",
       "       [-1.27758257],\n",
       "       [-1.30860042],\n",
       "       ...,\n",
       "       [-0.52870812],\n",
       "       [-0.24441601],\n",
       "       [-0.07864238]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd['edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9598, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd[0]['edges'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16998, 1)\n",
      "(16998, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_values['target'].edges.shape)\n",
    "print(output.edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9598, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdd[0]['edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.57434502],\n",
       "       [-5.57434871],\n",
       "       [-5.5743279 ],\n",
       "       ...,\n",
       "       [-5.56945051],\n",
       "       [-5.57278358],\n",
       "       [-5.57282286]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd[0]['edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{1413, 777, 1034, 781, 909, 1176, 538, 410, 923, 1150, 414, 1306, 800, 935, 40, 173, 1581, 943, 1327, 1456, 182, 567, 1336, 1213, 1471, 1472, 1476, 78, 1486, 1106, 1619, 598, 343, 217, 227, 744, 1000, 749, 1649, 885, 506, 254}\n",
      "1634 1634\n"
     ]
    }
   ],
   "source": [
    "print(used_nodes <= orig_nodes)\n",
    "print(orig_nodes.difference(used_nodes))\n",
    "print(len(used_nodes), len(graph.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "print(G.Ri[343].nonzero())\n",
    "print(G.Ro[343].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.graph import graph_to_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-64129339eb8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Ri' is not defined"
     ]
    }
   ],
   "source": [
    "S = graph_to_sparse(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    1    1 ... 1675 1675 1675] [   0  123  135 ... 2744 2872 2893]\n"
     ]
    }
   ],
   "source": [
    "print(S['Ri_rows'], S['Ri_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1675 1675 1675] [   0    1    2 ... 3545 3546 3547]\n"
     ]
    }
   ],
   "source": [
    "print(S['Ro_rows'], S['Ro_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),)\n"
     ]
    }
   ],
   "source": [
    "print(G.Ri[0].nonzero())\n",
    "print(G.Ro[0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.Ro[:, 0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.Ri[:, 0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter some hits and build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare import select_hits\n",
    "from prepare import split_detector_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evtid = 1\n",
    "pt_min = 1\n",
    "phi_slope_max = 0.001\n",
    "z0_max = 200\n",
    "n_phi_sections = 4\n",
    "n_eta_sections = 1\n",
    "filtered_hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=evtid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_range = (-np.pi, np.pi)\n",
    "eta_range = [-5, 5]\n",
    "phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)\n",
    "eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)\n",
    "hits_sections = split_detector_sections(filtered_hits, phi_edges, eta_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_names = ['r', 'phi', 'z']\n",
    "node_feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define adjacent layers\n",
    "n_det_layers = 10\n",
    "l = np.arange(n_det_layers)\n",
    "layer_pairs = np.stack([l[:-1], l[1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on one section now\n",
    "isection = 0\n",
    "section_hits = hits_sections[isection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrackPy3.6",
   "language": "python",
   "name": "trackkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

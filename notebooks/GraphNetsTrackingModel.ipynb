{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/bin/python\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets.graph import load_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare network-x graphs using hitsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/global/cscratch1/sd/xju/heptrkx/data/hitgraphs_001/event00000{}_g{:03d}.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(in_node, out_node):\n",
    "    # input are the features of incoming and outgoing nodes\n",
    "    # they are ordered as [r, phi, z]\n",
    "    in_r, in_phi, _   = in_node\n",
    "    out_r, out_phi, _ = out_node\n",
    "    in_x = in_r * np.cos(in_phi)\n",
    "    in_y = in_r * np.sin(in_phi)\n",
    "    out_x = out_r * np.cos(out_phi)\n",
    "    out_y = out_r * np.sin(out_phi)\n",
    "    return np.sqrt((in_x - out_x)**2 + (in_y - out_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_graph_from_hitsgraph(ievt, isec):\n",
    "    ## ievt start from 1000\n",
    "    file_name = base_dir.format(ievt, isec)\n",
    "    return hitsgraph_to_networkx_graph(load_graph(file_name))\n",
    "\n",
    "\n",
    "def hitsgraph_to_networkx_graph(G):\n",
    "    n_nodes, n_edges = G.Ri.shape\n",
    "    \n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    ## add nodes\n",
    "    for i in range(n_nodes):\n",
    "        graph.add_node(i, pos=G.X[i], solution=0.0)\n",
    "    \n",
    "    for iedge in range(n_edges):\n",
    "        in_node_id  = G.Ri[:, iedge].nonzero()[0][0]\n",
    "        out_node_id = G.Ro[:, iedge].nonzero()[0][0]\n",
    "\n",
    "        # distance as features\n",
    "        in_node_features  = G.X[in_node_id]\n",
    "        out_node_features = G.X[out_node_id]\n",
    "        distance = get_edge_features(in_node_features, out_node_features)\n",
    "        # add edges, bi-directions\n",
    "        graph.add_edge(in_node_id, out_node_id, distance=distance, solution=G.y[iedge])\n",
    "        graph.add_edge(out_node_id, in_node_id, distance=distance, solution=G.y[iedge])\n",
    "        # add \"solution\" to nodes\n",
    "        graph.node[in_node_id].update(solution=G.y[iedge])\n",
    "        graph.node[out_node_id].update(solution=G.y[iedge])\n",
    "        \n",
    "        \n",
    "    # add global features, not used for now\n",
    "    graph.graph['features'] = np.array([0.])\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def graph_to_input_target(graph):\n",
    "    def create_feature(attr, fields):\n",
    "        return np.hstack([np.array(attr[field], dtype=float) for field in fields])\n",
    "    \n",
    "    input_node_fields = (\"pos\",)\n",
    "    input_edge_fields = (\"distance\",)\n",
    "    target_node_fields = (\"solution\",)\n",
    "    target_edge_fields = (\"solution\",)\n",
    "    \n",
    "    input_graph = graph.copy()\n",
    "    target_graph = graph.copy()\n",
    "    \n",
    "    for node_index, node_feature in graph.nodes(data=True):\n",
    "        input_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, input_node_fields)\n",
    "        )\n",
    "        target_graph.add_node(\n",
    "            node_index, features=create_feature(node_feature, target_node_fields)\n",
    "        )\n",
    "        \n",
    "    for receiver, sender, features in graph.edges(data=True):\n",
    "        input_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, input_edge_fields)\n",
    "        )\n",
    "        target_graph.add_edge(\n",
    "            sender, receiver, features=create_feature(features, target_edge_fields)\n",
    "        )\n",
    "        \n",
    "    input_graph.graph['features'] = input_graph.graph['features'] = np.array([0.0])\n",
    "    return input_graph, target_graph\n",
    "\n",
    "\n",
    "def generate_input_target(n_graphs, start_evt_id=1000):\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    for i in range(n_graphs):\n",
    "        evt_id = start_evt_id + i\n",
    "        isec = 0\n",
    "        graph = networkx_graph_from_hitsgraph(evt_id, isec)\n",
    "        input_graph, output_graph = graph_to_input_target(graph)\n",
    "        input_graphs.append(input_graph)\n",
    "        target_graphs.append(output_graph)\n",
    "    return input_graphs, target_graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use interaction network from modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_nets import modules\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "import sonnet as snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.\n",
    "\n",
    "\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "\n",
    "  Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "  \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "  def __init__(self, name=\"MLPGraphIndependent\"):\n",
    "    super(MLPGraphIndependent, self).__init__(name=name)\n",
    "    with self._enter_variable_scope():\n",
    "      self._network = modules.GraphIndependent(\n",
    "          edge_model_fn=make_mlp_model,\n",
    "          node_model_fn=make_mlp_model,\n",
    "          global_model_fn=None)\n",
    "\n",
    "  def _build(self, inputs):\n",
    "    return self._network(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class SegmentClassifier(snt.AbstractModule):\n",
    "\n",
    "  def __init__(self, name=\"SegmentClassifier\"):\n",
    "    super(SegmentClassifier, self).__init__(name=name)\n",
    "\n",
    "    self._encoder = MLPGraphIndependent()\n",
    "    self._core = modules.InteractionNetwork(\n",
    "        edge_model_fn=make_mlp_model,\n",
    "        node_model_fn=make_mlp_model,\n",
    "        reducer=tf.unsorted_segment_prod\n",
    "    )\n",
    "\n",
    "    # Transforms the outputs into the appropriate shapes.\n",
    "    edge_output_size = 1\n",
    "#     edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "    edge_fn =lambda: snt.nets.MLP([edge_output_size], activation=tf.nn.tanh, name='edge_output')\n",
    "\n",
    "    with self._enter_variable_scope():\n",
    "      self._output_transform = modules.GraphIndependent(edge_fn, None, None)\n",
    "\n",
    "  def _build(self, input_op, num_processing_steps):\n",
    "    latent = self._encoder(input_op)\n",
    "\n",
    "    output_ops = []\n",
    "    for _ in range(num_processing_steps):\n",
    "        core_input = latent\n",
    "        latent = self._core(core_input)\n",
    "        output_ops.append(self._output_transform(latent))\n",
    "    return output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Loss functions and Feed-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global event_track\n",
    "event_track = 1000\n",
    "def create_feed_dict(batch_size, input_ph, target_ph):\n",
    "    global event_track\n",
    "    inputs, targets = generate_input_target(batch_size, event_track)\n",
    "    event_track += batch_size\n",
    "\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "#         print(event_track)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_ops(target_op, output_ops):\n",
    "    # only use edges\n",
    "    loss_ops = [\n",
    "        tf.losses.sigmoid_cross_entropy(target_op.edges, output_op.edges)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "  \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "  return [utils_tf.make_runnable_in_session(a) for a in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_matrics(target, output):\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    \n",
    "    test_target = []\n",
    "    test_pred = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        test_target.append(td['edges'])\n",
    "        test_pred.append(od['edges'])\n",
    "    \n",
    "    test_target = np.concatenate(test_target, axis=0)\n",
    "    test_pred   = np.concatenate(test_pred,   axis=0)\n",
    "    \n",
    "    thresh = 0.5\n",
    "    y_pred, y_true = (test_pred > thresh), (test_target > thresh)\n",
    "    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = n_graphs = 2\n",
    "num_training_iterations = 10000\n",
    "num_processing_steps_tr = 4  ## level of message-passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graphs, target_graphs = generate_input_target(n_graphs)\n",
    "input_ph  = utils_tf.placeholders_from_networkxs(input_graphs, force_dynamic_num_graphs=True)\n",
    "target_ph = utils_tf.placeholders_from_networkxs(target_graphs, force_dynamic_num_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\n",
      "# 00005, T 26.0, Ltr 0.6362, Lge 0.6095, Precision 0.5758, Recall 0.0489\n",
      "# 00010, T 51.6, Ltr 0.6143, Lge 0.6387, Precision 0.5152, Recall 0.0526\n",
      "# 00018, T 76.2, Ltr 0.6131, Lge 0.5928, Precision 0.5609, Recall 0.0246\n",
      "# 00023, T 96.0, Ltr 0.6434, Lge 0.6119, Precision 0.6460, Recall 0.0243\n",
      "# 00029, T 117.0, Ltr 0.6223, Lge 0.5882, Precision 0.7229, Recall 0.0167\n",
      "# 00035, T 138.5, Ltr 0.6184, Lge 0.6019, Precision 0.7286, Recall 0.0145\n",
      "# 00041, T 160.7, Ltr 0.6109, Lge 0.5993, Precision 0.7368, Recall 0.0138\n",
      "# 00047, T 183.6, Ltr 0.6025, Lge 0.6319, Precision 0.7677, Recall 0.0209\n",
      "# 00053, T 205.9, Ltr 0.5919, Lge 0.6179, Precision 0.7569, Recall 0.0160\n",
      "# 00059, T 226.5, Ltr 0.6028, Lge 0.6084, Precision 0.8364, Recall 0.0171\n",
      "# 00066, T 249.5, Ltr 0.5623, Lge 0.6173, Precision 0.8354, Recall 0.0173\n",
      "# 00072, T 270.2, Ltr 0.5933, Lge 0.5995, Precision 0.8424, Recall 0.0158\n",
      "# 00078, T 290.7, Ltr 0.6027, Lge 0.6004, Precision 0.7474, Recall 0.0166\n",
      "# 00085, T 311.9, Ltr 0.6077, Lge 0.5879, Precision 0.8067, Recall 0.0134\n",
      "# 00092, T 333.1, Ltr 0.6091, Lge 0.6082, Precision 0.7844, Recall 0.0150\n",
      "# 00099, T 355.3, Ltr 0.5984, Lge 0.5975, Precision 0.8037, Recall 0.0211\n",
      "# 00106, T 376.8, Ltr 0.5843, Lge 0.6082, Precision 0.7829, Recall 0.0164\n",
      "# 00112, T 398.4, Ltr 0.5771, Lge 0.5898, Precision 0.8357, Recall 0.0194\n",
      "# 00119, T 420.4, Ltr 0.5810, Lge 0.5954, Precision 0.7406, Recall 0.0173\n",
      "# 00126, T 442.3, Ltr 0.6093, Lge 0.5533, Precision 0.7745, Recall 0.0152\n",
      "# 00132, T 462.9, Ltr 0.5827, Lge 0.5770, Precision 0.7838, Recall 0.0164\n",
      "# 00139, T 484.4, Ltr 0.5998, Lge 0.5670, Precision 0.7729, Recall 0.0190\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0badda6744a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mlast_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   train_values = sess.run({\n\u001b[1;32m     40\u001b[0m       \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-db5bc38f93c5>\u001b[0m in \u001b[0;36mcreate_feed_dict\u001b[0;34m(batch_size, input_ph, target_ph)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mevent_track\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_input_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mevent_track\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7dfd3a33ba74>\u001b[0m in \u001b[0;36mgenerate_input_target\u001b[0;34m(n_graphs, start_evt_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0misec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworkx_graph_from_hitsgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0minput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_to_input_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0minput_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtarget_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7dfd3a33ba74>\u001b[0m in \u001b[0;36mgraph_to_input_target\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0minput_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtarget_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_feature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         G.add_edges_from((u, v, datadict.copy())\n\u001b[0;32m-> 1532\u001b[0;31m                          \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m                          for v, datadict in nbrs.items())\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/project/projectdirs/atlas/xju/miniconda3/envs/py3.6/lib/python3.6/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Reset session  { form-width: \"30%\" }\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "\n",
    "\n",
    "#@title Run training  { form-width: \"30%\" }\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), \"\n",
    "      \"Precision, \"\n",
    "      \"Recall\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"target\": target_ph,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  }, feed_dict=feed_dict)\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    feed_dict = create_feed_dict(batch_size, input_ph, target_ph)\n",
    "    test_values = sess.run({\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    }, feed_dict=feed_dict)\n",
    "    correct_tr, solved_tr = computer_matrics(\n",
    "        test_values[\"target\"], test_values[\"outputs\"][-1])\n",
    "    elapsed = time.time() - start_time\n",
    "    losses_tr.append(train_values[\"loss\"])\n",
    "    corrects_tr.append(correct_tr)\n",
    "    solveds_tr.append(solved_tr)\n",
    "    logged_iterations.append(iteration)\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall\"\n",
    "          \" {:.4f}\".format(\n",
    "              iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"],\n",
    "              correct_tr, solved_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrackPy3.6",
   "language": "python",
   "name": "trackkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

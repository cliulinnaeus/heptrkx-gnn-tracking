#!/usr/bin/env python
def process(input_info, selected_hits_angle, output_pairs_dir):
    layer_pair, ii = input_info
    out_name = os.path.join(output_pairs_dir, 'pair{:03d}.h5'.format(ii))
    segments = list(utils_mldata.create_segments(selected_hits_angle, [layer_pair]))
    with pd.HDFStore(out_name) as store:
            store['data'] = segments[0]


if __name__ == "__main__":
    import os
    import argparse

    parser = argparse.ArgumentParser(description='make pairs for given evtid')
    add_arg = parser.add_argument
    add_arg('config', type=str, help='data configuration, configs/data.yaml')
    add_arg('evtid', type=int, help='event id')
    add_arg('--n-pids', type=int, help='how many particles should be used',
            default=-1)
    add_arg('--workers', type=int, help='workers', default=1)
    add_arg('-q', '--quiet', action='store_true', help='quiet mode')

    args = parser.parse_args()
    n_workers = args.workers

    from heptrkx import layer_pairs, select_pair_layers, load_yaml

    config = load_yaml(args.config)
    data_dir = config['track_ml']['dir']
    black_list_dir = config['track_ml']['blacklist_dir']
    det_dir  = config['track_ml']['detector']
    base_dir = config['doublets_for_training']['base_dir']
    output_dir = os.path.join(base_dir, config['doublets_for_training']['all_pairs'])
    layers = config['doublets_for_graph']['layers']

    evtid = args.evtid
    n_pids = args.n_pids
    quiet = args.quiet

    sel_layer_id = select_pair_layers(layers)

    output_pairs_dir = os.path.join(output_dir, 'evt{}'.format(evtid))
    os.makedirs(output_pairs_dir, exist_ok=True)
    ## remove pairs already produced...
    sel_layer_id = [ii for ii in sel_layer_id if not os.path.exists(os.path.join(output_pairs_dir, 'pair{:03d}.h5'.format(ii)))]
    if len(sel_layer_id) < 1:
        exit(0)

    pp_layers_info = [(layer_pairs[ii], ii) for ii in sel_layer_id]




    from heptrkx.preprocess import utils_mldata
    results = utils_mldata.read(data_dir, black_list_dir, evtid)
    if results is None:
        exit()
    else:
        hits, particles, truth, cells = results

    reco_pids = utils_mldata.reconstructable_pids(particles, truth)
    from heptrkx.nx_graph import utils_data
    import numpy as np
    import pandas as pd

    # noise included!
    hh = utils_data.merge_truth_info_to_hits(hits, particles, truth)
    unique_pids = np.unique(hh['particle_id'])
    hh = hh[hh.layer.isin(layers)]

    if n_pids > 0:
        selected_pids = np.random.choice(unique_pids, size=n_pids)
        selected_hits = hh[hh.particle_id.isin(selected_pids)].assign(evtid=evtid)
    else:
        selected_hits = hh.assign(evtid=evtid)


    if not quiet:
        print("---- make pairs for event {} ----".format(evtid))
        print("{} particles {} reconstructable".format(
            unique_pids.shape[0],
            reco_pids.shape[0]))
        print("\tuses {} Layers.".format(len(layers)))
        print("\tTotal {} Layer Pairs.".format(len(sel_layer_id)))
        print("\tuses {} Workers:".format(n_workers))

    from heptrkx.nx_graph import transformation
    module_getter = utils_mldata.module_info(det_dir)

    from functools import partial

    local_angles = utils_mldata.cell_angles(selected_hits, module_getter, cells)
    selected_hits_angle = selected_hits.merge(local_angles, on='hit_id', how='left')


    import multiprocessing as mp
    with mp.Pool(processes=n_workers) as pool:
        pp_func=partial(process, selected_hits_angle=selected_hits_angle,
                        output_pairs_dir=output_pairs_dir)
        pool.map(pp_func, pp_layers_info)

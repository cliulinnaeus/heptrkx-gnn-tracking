#!/usr/bin/env python
"""
Data preparation script for GNN tracking.

This script processes the TrackML dataset and produces graph data on disk.
"""

# System
import os
import argparse
import logging
import multiprocessing as mp
from functools import partial

# Externals
import yaml
import numpy as np
import pandas as pd

from heptrkx.preprocess import utils_mldata
from heptrkx.nx_graph import utils_data
from heptrkx import load_yaml, pairwise, select_pair_layers, layer_pairs

def process(evtid, config_dir):
    config = load_yaml(config_dir)
    data_dir = config['track_ml']['dir']
    det_dir  = config['track_ml']['detector']

    # Load the data
    results = utils_mldata.read(data_dir, evtid)
    if results is None:
        print("[{}] cannot find event {}".format(prog, evtid))
        exit(1)
    else:
        hits, particles, truth, cells = results

    # only hits in barrel
    logging.info('Event %i, selecting hits' % evtid)
    hits = utils_data.merge_truth_info_to_hits(hits, particles, truth)
    layers = config['doublets_from_cuts']['layers']
    sel_layer_id = select_pair_layers(layers)
    sel_layer_pairs = [layer_pairs[ii] for ii in sel_layer_id]

    hits = hits[hits.layer.isin(layers)].assign(evtid=evtid)
    module_getter = utils_mldata.module_info(det_dir)

    # add local angles to hits
    local_angles = utils_mldata.cell_angles(hits, module_getter, cells)
    hits = hits.merge(local_angles, on='hit_id', how='left')

    # Graph features and scale

    phi_slope_max = config['doublets_from_cuts']['phi_slope_max']
    z0_max = config['doublets_from_cuts']['z0_max']

    output_dir = os.path.join(config['doublets_from_cuts']['selected'], 'evt{}'.format(evtid))
    os.makedirs(output_dir, exist_ok=True)

    for pair_id in sel_layer_id:
        pairs = layer_pairs[pair_id]
        outname = os.path.join(output_dir, "pair{:03d}.h5".format(pair_id))
        if os.path.exists(outname):
            continue

        df = list(utils_mldata.create_segments(hits, [pairs]))
        with pd.HDFStore(outname) as store:
            store['data'] = df[0]


if __name__ == "__main__":
    import os
    import argparse

    parser = argparse.ArgumentParser(description='Keras train pairs for each layer-pairs')
    add_arg = parser.add_argument
    add_arg('config', type=str, help='data configuration, configs/data.yaml')
    add_arg('--workers', type=int, help='workers', default=1)
    add_arg('--mpi', action='store_true', help='use MPI')

    args = parser.parse_args()
    config_dir = args.config
    n_workers = args.workers
    use_mpi = args.mpi

    if use_mpi:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
            size = comm.Get_size()
            rank = comm.Get_rank()
            print("World size:", size, ", rank:", rank)
        except ImportError:
            rank = 0
            size = 1
    else:
        rank = 0
        size =1


    if rank == 0:
        assert(os.path.exists(config_dir))
        config = load_yaml(config_dir)
        mk_cfg = config['doublets_from_cuts']
        evtids = mk_cfg['evtid']
        output_dir = os.path.expandvars(mk_cfg['selected'])
        os.makedirs(output_dir, exist_ok=True)
        if type(evtids) is str:
            evtids = list_from_str(evtids)
        else:
            evtids = [evtids]

        ## split evtids based on the world-size
        evtids = [x.tolist() for x in np.array_split(evtids, size)]
    else:
        evtids = None

    if use_mpi:
        comm.Barrier()
        evtids = comm.scatter(evtids, root=0)
    else:
        evtids = evtids[0]

    import multiprocessing as mp
    from functools import partial

    print("rank({}) {} workers:".format(rank, n_workers))
    print("rank({}) {} events:".format(rank, len(evtids)))

    with  mp.Pool(processes=n_workers) as pool:
        pp_func = partial(process, config_dir=config_dir)
        pool.map(pp_func, evtids)

#!/usr/bin/env python
"""Data preparation script for GNN tracking.
Process the dataset and save doublets to disk.
"""

# System
import os
import argparse
import multiprocessing as mp
from functools import partial

# Externals
import yaml
import numpy as np
import pandas as pd
import h5py


from heptrkx.preprocess import utils_mldata
from heptrkx.nx_graph import utils_data
from heptrkx import load_yaml, pairwise, select_pair_layers, layer_pairs, list_from_str

from heptrkx import seeding
from heptrkx.master import Event

def process(evtid, config_dir, remove_duplicated_hits=False):
    """
    Produce doublets from hits only from the specified layers
    using the cuts on phi slope and z0.
    Outputs are the pair indexes in HDF5 format.
    Efficiency and Purity are the attributes of the dataset.
    """
    config = load_yaml(config_dir)
    data_dir = config['track_ml']['dir']
    layers = config['doublets_from_cuts']['layers']
    sel_layer_id = select_pair_layers(layers)

    event = Event(data_dir)
    if not event.read(evtid, merge_truth=True):
        print("[{}] cannot find event {}".format(prog, evtid))
        exit(1)
    hits = event.filter_hits(layers)
    if remove_duplicated_hits:
        hits = event.remove_duplicated_hits()

    # Graph features and scale
    phi_slope_max = config['doublets_from_cuts']['phi_slope_max']
    z0_max = config['doublets_from_cuts']['z0_max']

    output_dir = os.path.join(config['doublets_from_cuts']['selected'], 'evt{}'.format(evtid))
    os.makedirs(output_dir, exist_ok=True)

    for pair_id in sel_layer_id:
        pairs = layer_pairs[pair_id]
        outname = os.path.join(output_dir, "pair{:03d}.h5".format(pair_id))
        if os.path.exists(outname):
            continue

        df = seeding.create_segments(hits, pairs)
        n_total = df.shape[0]
        n_true = df[df.true].shape[0]
        df = df[(df.phi_slope.abs() < phi_slope_max) & (df.z0.abs() < z0_max)]
        n_selected_true = df[df.true].shape[0]
        efficiency = n_selected_true/n_true
        purity = n_selected_true/df.shape[0]

        with pd.HDFStore(outname, 'w') as store:
            store['data'] = df
            store['info'] = pd.Series([efficiency, purity], index=['efficiency', 'purity'])

        # save selected pairs into HDF5 file
        # instead of HDFStore from pandas, I use HDF5 so to save some meta-info
        #with h5py.File(outname, 'w') as f:
        #    dest = f.create_dataset('data', data=df.values)
        #    dest.attrs['columns'] = np.string_(df.columns)
        #    dest.attrs['efficiency'] = efficiency
        #    dest.attrs['purity'] = purity

        print("[{}]: event {}, pair {}({}, {}),\
              efficiency {:.2f} and purity {:.2f}".format(
                  os.getpid(), evtid,
                  pair_id, pairs[0], pairs[1],
                  efficiency, purity
              ))


if __name__ == "__main__":
    import os
    import argparse

    parser = argparse.ArgumentParser(description='make doublets using cut-based methods')
    add_arg = parser.add_argument
    add_arg('config', type=str, help='data configuration, configs/data.yaml')
    add_arg('--workers', type=int, help='workers', default=1)
    add_arg('--mpi', action='store_true', help='use MPI')
    add_arg('--noduplicate', action='store_true', help='remove duplicate hits')

    args = parser.parse_args()
    config_dir = args.config
    n_workers = args.workers
    use_mpi = args.mpi
    no_dup = args.noduplicate

    if use_mpi:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
            size = comm.Get_size()
            rank = comm.Get_rank()
            print("World size:", size, ", rank:", rank)
        except ImportError:
            rank = 0
            size = 1
    else:
        rank = 0
        size =1


    if rank == 0:
        assert(os.path.exists(config_dir))
        config = load_yaml(config_dir)
        mk_cfg = config['doublets_from_cuts']
        evtids = mk_cfg['evtid']
        output_dir = os.path.expandvars(mk_cfg['selected'])
        os.makedirs(output_dir, exist_ok=True)
        if type(evtids) is str:
            evtids = list_from_str(evtids)
        else:
            evtids = [evtids]

        ## split evtids based on the world-size
        evtids = [x.tolist() for x in np.array_split(evtids, size)]
    else:
        evtids = None

    if use_mpi:
        comm.Barrier()
        evtids = comm.scatter(evtids, root=0)
    else:
        evtids = evtids[0]

    import multiprocessing as mp
    from functools import partial

    print("rank({}) {} workers deal with {} events.".format(rank, n_workers, len(evtids)))

    with  mp.Pool(processes=n_workers) as pool:
        pp_func = partial(process, config_dir=config_dir, remove_duplicated_hits=no_dup)
        pool.map(pp_func, evtids)

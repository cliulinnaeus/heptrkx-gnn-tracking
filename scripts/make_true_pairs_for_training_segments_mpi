#!/usr/bin/env python


def process(input_info, selected_hits_angle, output_dir):
    layer_pair, ii = input_info
    segments = list(utils_mldata.create_segments(selected_hits_angle, [layer_pair], only_true=True))
    os.makedirs(output_dir, exist_ok=True)

    with pd.HDFStore(os.path.join(output_dir, 'pair{:03d}.h5'.format(ii))) as store:
            store['data'] = segments[0]


if __name__ == "__main__":
    import os
    import glob
    import re

    import argparse

    parser = argparse.ArgumentParser(description='produce true pairs using MPI')
    add_arg = parser.add_argument
    add_arg('config', type=str, help='data configuration, configs/data.yaml')

    args = parser.parse_args()

    from heptrkx import layer_pairs, load_yaml
    config = load_yaml(args.config)

    data_dir = os.path.expandvars(config['track_ml']['dir'])
    black_list_dir = os.path.expandvars(config['track_ml']['blacklist_dir'])
    det_dir  = os.path.expandvars(config['track_ml']['detector'])
    base_output_dir = os.path.expandvars(config['true_hits']['dir'])

    try:
        from mpi4py import MPI
        comm = MPI.COMM_WORLD
        size = comm.Get_size()
        rank = comm.Get_rank()
        print("World size:", size, ", rank:", rank)
        use_mpi = True
    except ImportError:
        rank = 0
        size = 1
        use_mpi = False

    from heptrkx.preprocess import utils_mldata
    import numpy as np
    import pandas as pd
    from heptrkx.nx_graph import utils_data

    if rank == 0:
        all_files = glob.glob(os.path.join(data_dir, '*-hits.csv'))
        evt_ids = np.sort([int(re.search('event00000([0-9]*)-hits.csv',
                                         os.path.basename(x)).group(1))
                           for x in all_files])
        n_events = len(evt_ids)
        print("Total Events:", n_events)

        # remove existing ones
        all_existing_files = glob.glob(os.path.join(base_output_dir, '*'))
        existing_evt_ids = set([int(os.path.basename(x)[3:]) for x in all_existing_files])
        set_evt_ids = set(evt_ids.tolist())
        evt_ids = np.array(list(set_evt_ids.difference(existing_evt_ids)))
        print("Left Events:", len(evt_ids))

        ## check existing evt-ids
        evt_ids = [x.tolist() for x in np.array_split(evt_ids, size)]

    else:
        evt_ids = None

    if use_mpi:
        comm.Barrier()
        evt_ids = comm.scatter(evt_ids, root=0)
    else:
        evt_ids = evt_ids[0]

    from heptrkx.nx_graph import transformation
    module_getter = utils_mldata.module_info(det_dir)

    from functools import partial
    import multiprocessing as mp

    try:
        n_workers = int(os.getenv('SLURM_CPUS_PER_TASK'))
    except (ValueError, TypeError):
        n_workers = 1

    print(rank, "# workers:", n_workers)
    print(rank, "# evts:", len(evt_ids))

    for evtid in evt_ids:
        output_dir = os.path.join(base_output_dir, 'evt{}'.format(evtid))
        if os.path.exists(output_dir):
            continue
        hits, particles, truth, cells = utils_mldata.read(data_dir, black_list_dir, evtid)

        reco_pids = utils_mldata.reconstructable_pids(particles, truth)

        # noise included!
        hh = utils_data.merge_truth_info_to_hits(hits, truth, particles)
        unique_pids = np.unique(hh['particle_id'])
        print("Event", evtid, "Number of particles:", unique_pids.shape, reco_pids.shape)

        n_pids = len(unique_pids)
        selected_pids = np.random.choice(unique_pids, size=n_pids)
        selected_hits = hh[hh.particle_id.isin(selected_pids)].assign(evtid=evtid)

        local_angles = utils_mldata.cell_angles(selected_hits, module_getter, cells)
        selected_hits_angle = selected_hits.merge(local_angles, on='hit_id', how='left')

        pp_layers_info = [(x, ii) for ii,x in enumerate(layer_pairs)]

        with mp.Pool(processes=n_workers) as pool:
            pp_func=partial(process, selected_hits_angle=selected_hits_angle, output_dir=output_dir)
            pool.map(pp_func, pp_layers_info)

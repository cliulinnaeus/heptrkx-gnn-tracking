#!/usr/bin/env python

import pandas as pd
import numpy as np
import yaml

def load_seg(file_name):
    with pd.HDFStore(file_name) as store:
        df = store.get('data')

    return df[df.selected]

def process(evtid, config_dir):
    with open(config_dir) as f:
        config = yaml.load(f)

    mk_cfg = config['doublets_for_graph']

    pairs_selected_dir = os.path.expandvars(mk_cfg['selected'])
    pairs_input_dir = os.path.join(pairs_selected_dir, "evt{}".format(evtid))
    layers = mk_cfg['layers']
    min_hits = mk_cfg['min_hits']


    # load event
    from heptrkx.preprocess import utils_mldata
    hits, particles, truth, cells = utils_mldata.read_event(evtid, config_dir, info=True)
    hits = utils_data.merge_truth_info_to_hits(hits, particles, truth)

    # select hits in predefined layers
    if layers[0] > 0:
        hits = hits[hits.layer.isin(layers)]

    particle_hits = particles.merge(hits, on='particle_id', how='left')
    dp = particle_hits[~np.isnan(particle_hits.hit_id)]

    ## good particles are the particles that have at least *min_hits* hits
    good_particles = (dp.groupby('particle_id')['hit_id'].count() > min_hits-1).index.to_numpy()
    particles = particles[particles.particle_id.isin(good_particles)]
    del particle_hits
    del dp

    print("event {} total hits: {}".format(evtid, hits.shape[0]))
    print("event {} total particles: {}".format(evtid, particles.shape[0]))

    # find layer pairs
    from heptrkx import layer_pairs, select_pair_layers
    sel_layer_id = select_pair_layers(layers)

    file_names = [os.path.join(pairs_input_dir, 'pair{:03d}.h5'.format(i))
                  for i in sel_layer_id]
    all_segments = [load_seg(file_name) for file_name in file_names]
    segments = pd.concat(all_segments, ignore_index=True)
    print("event {} total edges: ".format(evtid, segments.shape[0]))

    # make a graph and save to disk
    output_dir = mk_cfg['out_graph']

    graph = utils_data.segments_to_nx(
        hits, segments,
        sender_hitid_name='hit_id_in',
        receiver_hitid_name='hit_id_out',
        solution_name='true', use_digraph=True, bidirection=args.bidirection)

    out_name = os.path.join(output_dir, 'evt{}'.format(evtid))
    utils_io.save_networkx(graph, out_name)

if __name__ == "__main__":
    import os
    import argparse

    from heptrkx.nx_graph import utils_data
    from heptrkx.nx_graph import utils_io

    parser = argparse.ArgumentParser(description='Keras train pairs for each layer-pairs')
    add_arg = parser.add_argument
    add_arg('config', type=str, help='data configuration, configs/data.yaml')
    add_arg('--workers', type=int, help='number of threads', default=1)
    add_arg('--bidirection', action='store_true', help='use two-directions in graph')

    args = parser.parse_args()
    config_dir = args.config
    n_workers = args.workers

    try:
        from mpi4py import MPI
        comm = MPI.COMM_WORLD
        size = comm.Get_size()
        rank = comm.Get_rank()
        print("World size:", size, ", rank:", rank)
        use_mpi = True
    except ImportError:
        rank = 0
        size = 1
        use_mpi = False

    if rank == 0:
        import yaml
        assert(os.path.exists(config_dir))
        with open(config_dir) as f:
            config = yaml.load(f)


        mk_cfg = config['doublets_for_graph']
        evtids = mk_cfg['evtid']
        if type(evtids) is str:
            from heptrkx import list_from_str
            evtids = list_from_str(evtids)
        else:
            evtids = [evtids]

        ## split evtids based on the world-size
        evtids = [x.tolist() for x in np.array_split(evtids, size)]
    else:
        evtids = None

    if use_mpi:
        comm.Barrier()
        evtids = comm.scatter(evtids, root=0)
    else:
        evtids = evtids[0]

    import multiprocessing as mp
    from functools import partial

    print("rank({}) {} workers:".format(rank, n_workers))
    print("rank({}) {} events:".format(rank, len(evtids)))

    with  mp.Pool(processes=n_workers) as pool:
        pp_func = partial(process, config_dir=config_dir)
        pool.map(pp_func, evtids)
